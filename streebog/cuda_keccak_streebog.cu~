/*
 * Streebog GOST R 34.10-2012 CUDA implementation.
 *
 * https://tools.ietf.org/html/rfc6986
 * https://en.wikipedia.org/wiki/Streebog
 *
 * ==========================(LICENSE BEGIN)============================
 *
 * @author   Tanguy Pruvot - 2015
 * @author   Alexis Provos - 2016
 */

// Further improved with shared memory partial utilization
// Merged implementation to decrease shared memory's bottleneck
// Tested under CUDA7.5 toolkit for cp 5.0/5.2

#include "miner.h"

#include "cuda_helper.h"
#include "cuda_vectors.h"
#include "streebog_arrays.cuh"

__constant__ 
static uint2 keccak_round_constants[24] = {
		{ 0x00000001, 0x00000000 }, { 0x00008082, 0x00000000 },	{ 0x0000808a, 0x80000000 }, { 0x80008000, 0x80000000 },
		{ 0x0000808b, 0x00000000 }, { 0x80000001, 0x00000000 },	{ 0x80008081, 0x80000000 }, { 0x00008009, 0x80000000 },
		{ 0x0000008a, 0x00000000 }, { 0x00000088, 0x00000000 },	{ 0x80008009, 0x00000000 }, { 0x8000000a, 0x00000000 },
		{ 0x8000808b, 0x00000000 }, { 0x0000008b, 0x80000000 },	{ 0x00008089, 0x80000000 }, { 0x00008003, 0x80000000 },
		{ 0x00008002, 0x80000000 }, { 0x00000080, 0x80000000 },	{ 0x0000800a, 0x00000000 }, { 0x8000000a, 0x80000000 },
		{ 0x80008081, 0x80000000 }, { 0x00008080, 0x80000000 },	{ 0x80000001, 0x00000000 }, { 0x80008008, 0x80000000 }
};

__device__ __forceinline__
static void GOST_FS(const uint2 shared[8][256],const uint2 *const __restrict__ state,uint2* return_state){

	return_state[0] =  __ldg(&T02[__byte_perm(state[7].x,0,0x44440)])
			^ __ldg(&T12[__byte_perm(state[6].x,0,0x44440)])
			^ shared[2][__byte_perm(state[5].x,0,0x44440)]
			^ shared[3][__byte_perm(state[4].x,0,0x44440)]
			^ shared[4][__byte_perm(state[3].x,0,0x44440)]
			^ shared[5][__byte_perm(state[2].x,0,0x44440)]
			^ __ldg(&T72[__byte_perm(state[0].x,0,0x44440)])
			^ shared[6][__byte_perm(state[1].x,0,0x44440)];

	return_state[1] =  __ldg(&T02[__byte_perm(state[7].x,0,0x44441)])
			^ shared[2][__byte_perm(state[5].x,0,0x44441)]
			^ shared[3][__byte_perm(state[4].x,0,0x44441)]
			^ __ldg(&T12[__byte_perm(state[6].x,0,0x44441)])
 			^ shared[4][__byte_perm(state[3].x,0,0x44441)]
			^ shared[5][__byte_perm(state[2].x,0,0x44441)]
			^ __ldg(&T72[__byte_perm(state[0].x,0,0x44441)])
			^ shared[6][__byte_perm(state[1].x,0,0x44441)];

	return_state[2] =  __ldg(&T02[__byte_perm(state[7].x,0,0x44442)])
			^ __ldg(&T12[__byte_perm(state[6].x,0,0x44442)])
			^ shared[2][__byte_perm(state[5].x,0,0x44442)]
			^ shared[3][__byte_perm(state[4].x,0,0x44442)]
			^ shared[4][__byte_perm(state[3].x,0,0x44442)]
			^ shared[5][__byte_perm(state[2].x,0,0x44442)]
			^ __ldg(&T72[__byte_perm(state[0].x,0,0x44442)])
			^ shared[6][__byte_perm(state[1].x,0,0x44442)];

	return_state[3] =  __ldg(&T02[__byte_perm(state[7].x,0,0x44443)])
			^ shared[1][__byte_perm(state[6].x,0,0x44443)]
			^ shared[2][__byte_perm(state[5].x,0,0x44443)]
			^ shared[3][__byte_perm(state[4].x,0,0x44443)]
			^ __ldg(&T42[__byte_perm(state[3].x,0,0x44443)])
			^ shared[5][__byte_perm(state[2].x,0,0x44443)]
			^ __ldg(&T72[__byte_perm(state[0].x,0,0x44443)])
			^ shared[6][__byte_perm(state[1].x,0,0x44443)];

	return_state[4] =  __ldg(&T02[__byte_perm(state[7].y,0,0x44440)])
			^ shared[1][__byte_perm(state[6].y,0,0x44440)]
			^ __ldg(&T22[__byte_perm(state[5].y,0,0x44440)])
			^ shared[3][__byte_perm(state[4].y,0,0x44440)]
			^ shared[4][__byte_perm(state[3].y,0,0x44440)]
			^ __ldg(&T62[__byte_perm(state[1].y,0,0x44440)])
			^ shared[5][__byte_perm(state[2].y,0,0x44440)]
			^ __ldg(&T72[__byte_perm(state[0].y,0,0x44440)]);

	return_state[5] = __ldg(&T02[__byte_perm(state[7].y,0,0x44441)])
			^ shared[2][__byte_perm(state[5].y,0,0x44441)]
			^ __ldg(&T12[__byte_perm(state[6].y,0,0x44441)])
			^ shared[3][__byte_perm(state[4].y,0,0x44441)]
			^ shared[4][__byte_perm(state[3].y,0,0x44441)]
			^ shared[5][__byte_perm(state[2].y,0,0x44441)]
			^ __ldg(&T62[__byte_perm(state[1].y,0,0x44441)])
			^ __ldg(&T72[__byte_perm(state[0].y,0,0x44441)]);

	return_state[6] =  __ldg(&T02[__byte_perm(state[7].y,0,0x44442)])
			^ shared[1][__byte_perm(state[6].y,0,0x44442)]
			^ shared[2][__byte_perm(state[5].y,0,0x44442)]
			^ shared[3][__byte_perm(state[4].y,0,0x44442)]
			^ shared[4][__byte_perm(state[3].y,0,0x44442)]
			^ shared[5][__byte_perm(state[2].y,0,0x44442)]
			^ __ldg(&T62[__byte_perm(state[1].y,0,0x44442)])
			^ __ldg(&T72[__byte_perm(state[0].y,0,0x44442)]);

	return_state[7] =  __ldg(&T02[__byte_perm(state[7].y,0,0x44443)])
			^ __ldg(&T12[__byte_perm(state[6].y,0,0x44443)])
			^ shared[2][__byte_perm(state[5].y,0,0x44443)]
			^ shared[3][__byte_perm(state[4].y,0,0x44443)]
			^ shared[4][__byte_perm(state[3].y,0,0x44443)]
			^ shared[5][__byte_perm(state[2].y,0,0x44443)]
			^ __ldg(&T62[__byte_perm(state[1].y,0,0x44443)])
			^ __ldg(&T72[__byte_perm(state[0].y,0,0x44443)]);
}

__device__ __forceinline__
static void GOST_FS_LDG(const uint2 shared[8][256],const uint2 *const __restrict__ state,uint2* return_state){

	return_state[0] =  __ldg(&T02[__byte_perm(state[7].x,0,0x44440)])
			^ __ldg(&T12[__byte_perm(state[6].x,0,0x44440)])
			^ shared[2][__byte_perm(state[5].x,0,0x44440)]
			^ shared[3][__byte_perm(state[4].x,0,0x44440)]
			^ shared[4][__byte_perm(state[3].x,0,0x44440)]
			^ shared[5][__byte_perm(state[2].x,0,0x44440)]
			^ shared[6][__byte_perm(state[1].x,0,0x44440)]
			^ __ldg(&T72[__byte_perm(state[0].x,0,0x44440)]);

	return_state[1] =  __ldg(&T02[__byte_perm(state[7].x,0,0x44441)])
			^ shared[1][__byte_perm(state[6].x,0,0x44441)]
			^ shared[2][__byte_perm(state[5].x,0,0x44441)]
			^ shared[3][__byte_perm(state[4].x,0,0x44441)]
			^ shared[4][__byte_perm(state[3].x,0,0x44441)]
			^ shared[5][__byte_perm(state[2].x,0,0x44441)]
			^ shared[6][__byte_perm(state[1].x,0,0x44441)] 
			^ __ldg(&T72[__byte_perm(state[0].x,0,0x44441)]);

	return_state[2] =  __ldg(&T02[__byte_perm(state[7].x,0,0x44442)])
			^ __ldg(&T12[__byte_perm(state[6].x,0,0x44442)])
			^ shared[2][__byte_perm(state[5].x,0,0x44442)]
			^ shared[3][__byte_perm(state[4].x,0,0x44442)]
			^ shared[4][__byte_perm(state[3].x,0,0x44442)]
			^ shared[5][__byte_perm(state[2].x,0,0x44442)]
			^ shared[6][__byte_perm(state[1].x,0,0x44442)]
			^ __ldg(&T72[__byte_perm(state[0].x,0,0x44442)]);

	return_state[3] =  __ldg(&T02[__byte_perm(state[7].x,0,0x44443)])
			^ __ldg(&T12[__byte_perm(state[6].x,0,0x44443)])
			^ shared[2][__byte_perm(state[5].x,0,0x44443)]
			^ shared[3][__byte_perm(state[4].x,0,0x44443)]
			^ shared[4][__byte_perm(state[3].x,0,0x44443)]
			^ shared[5][__byte_perm(state[2].x,0,0x44443)]
			^ __ldg(&T62[__byte_perm(state[1].x,0,0x44443)])
			^ __ldg(&T72[__byte_perm(state[0].x,0,0x44443)]);

	return_state[4] = __ldg(&T02[__byte_perm(state[7].y,0,0x44440)])
			^ shared[1][__byte_perm(state[6].y,0,0x44440)]
			^ __ldg(&T22[__byte_perm(state[5].y,0,0x44440)])
			^ shared[3][__byte_perm(state[4].y,0,0x44440)]
			^ shared[4][__byte_perm(state[3].y,0,0x44440)]
			^ shared[5][__byte_perm(state[2].y,0,0x44440)]
			^ __ldg(&T62[__byte_perm(state[1].y,0,0x44440)])
			^ __ldg(&T72[__byte_perm(state[0].y,0,0x44440)]);

	return_state[5] = __ldg(&T02[__byte_perm(state[7].y,0,0x44441)])
			^ __ldg(&T12[__byte_perm(state[6].y,0,0x44441)])
			^ shared[2][__byte_perm(state[5].y,0,0x44441)]
			^ shared[3][__byte_perm(state[4].y,0,0x44441)]
			^ shared[4][__byte_perm(state[3].y,0,0x44441)]
			^ shared[5][__byte_perm(state[2].y,0,0x44441)]
			^ __ldg(&T62[__byte_perm(state[1].y,0,0x44441)])
			^ __ldg(&T72[__byte_perm(state[0].y,0,0x44441)]);

	return_state[6] =  __ldg(&T02[__byte_perm(state[7].y,0,0x44442)])
			^ __ldg(&T12[__byte_perm(state[6].y,0,0x44442)])
			^ shared[2][__byte_perm(state[5].y,0,0x44442)]
			^ shared[3][__byte_perm(state[4].y,0,0x44442)]
			^ shared[4][__byte_perm(state[3].y,0,0x44442)]
			^ shared[5][__byte_perm(state[2].y,0,0x44442)]
			^ __ldg(&T62[__byte_perm(state[1].y,0,0x44442)])
			^ __ldg(&T72[__byte_perm(state[0].y,0,0x44442)]);

	return_state[7] =  __ldg(&T02[__byte_perm(state[7].y,0,0x44443)])
			^ shared[1][__byte_perm(state[6].y,0,0x44443)]
			^ __ldg(&T22[__byte_perm(state[5].y,0,0x44443)])
			^ shared[3][__byte_perm(state[4].y,0,0x44443)]
			^ shared[4][__byte_perm(state[3].y,0,0x44443)]
			^ shared[5][__byte_perm(state[2].y,0,0x44443)]
			^ __ldg(&T62[__byte_perm(state[1].y,0,0x44443)])
			^ __ldg(&T72[__byte_perm(state[0].y,0,0x44443)]);
}

__device__ __forceinline__
static void GOST_E12(const uint2 shared[8][256],uint2 *const __restrict__ K, uint2 *const __restrict__ state){

	uint2 t[ 8];
//	#pragma unroll 2
	for(int i=0; i<12; i++){
		GOST_FS(shared,state, t);
		
		#pragma unroll 8
		for(int j=0;j<8;j++)
			K[ j] ^= *(uint2*)&CC[i][j];
		
		#pragma unroll 8
		for(int j=0;j<8;j++)
			state[ j] = t[ j];
		
		GOST_FS_LDG(shared,K, t);

		#pragma unroll 8
		for(int j=0;j<8;j++)
			state[ j]^= t[ j];

		#pragma unroll 8
		for(int j=0;j<8;j++)
			K[ j] = t[ j];
	}
}

__device__ __forceinline__
static void keccak_kernel(uint2* s){
	uint2 u[5],t[5], v, w;

	/*theta*/
	t[ 0] = vectorize(devectorize(s[ 0])^devectorize(s[ 5]));
	t[ 1] = vectorize(devectorize(s[ 1])^devectorize(s[ 6]));
	t[ 2] = vectorize(devectorize(s[ 2])^devectorize(s[ 7]));
	t[ 3] = vectorize(devectorize(s[ 3])^devectorize(s[ 8]));
	t[ 4] = s[4];


	#pragma unroll 5
	for(int j=0;j<5;j++){
		u[ j] = ROL2(t[ j], 1);
	}
	
	s[ 4] = xor3x(s[ 4], t[3], u[ 0]);
	s[24] = s[19] = s[14] = s[ 9] = t[ 3] ^ u[ 0];

	s[ 0] = xor3x(s[ 0], t[4], u[ 1]);
	s[ 5] = xor3x(s[ 5], t[4], u[ 1]);
	s[20] = s[15] = s[10] = t[4] ^ u[ 1];

	s[ 1] = xor3x(s[ 1], t[0], u[ 2]);
	s[ 6] = xor3x(s[ 6], t[0], u[ 2]);
	s[21] = s[16] = s[11] = t[0] ^ u[ 2];
		
	s[ 2] = xor3x(s[ 2], t[1], u[ 3]);
	s[ 7] = xor3x(s[ 7], t[1], u[ 3]);
	s[22] = s[17] = s[12] = t[1] ^ u[ 3];
		
	s[ 3] = xor3x(s[ 3], t[2], u[ 4]);s[ 8] = xor3x(s[ 8], t[2], u[ 4]);
	s[23] = s[18] = s[13] = t[2] ^ u[ 4];
	v = s[1];
	s[1]  = ROL2(s[6], 44);
	s[6]  = ROL2(s[9], 20);
	s[9]  = ROL2(s[22], 61);
	s[22] = ROL2(s[14], 39);
	s[14] = ROL2(s[20], 18);
	s[20] = ROL2(s[2], 62);
	s[2]  = ROL2(s[12], 43);
	s[12] = ROL2(s[13], 25);
	s[13] = ROL8(s[19]);
	s[19] = ROR8(s[23]);
	s[23] = ROL2(s[15], 41);
	s[15] = ROL2(s[4], 27);
	s[4]  = ROL2(s[24], 14);
	s[24] = ROL2(s[21], 2);
	s[21] = ROL2(s[8], 55);
	s[8]  = ROL2(s[16], 45);
	s[16] = ROL2(s[5], 36);
	s[5]  = ROL2(s[3], 28);
	s[3]  = ROL2(s[18], 21);
	s[18] = ROL2(s[17], 15);
	s[17] = ROL2(s[11], 10);
	s[11] = ROL2(s[7], 6);
	s[7]  = ROL2(s[10], 3);
	s[10] = ROL2(v, 1);
	#pragma unroll 5
	for(int j=0;j<25;j+=5){
		v=s[j];w=s[j + 1];s[j] = chi(v,w,s[j+2]);s[j+1] = chi(w,s[j+2],s[j+3]);s[j+2]=chi(s[j+2],s[j+3],s[j+4]);s[j+3]=chi(s[j+3],s[j+4],v);s[j+4]=chi(s[j+4],v,w);
	}
	s[0] ^= keccak_round_constants[ 0];

	for (int i = 1; i < 24; i++) {
		/*theta*/
		#pragma unroll 5
		for(int j=0;j<5;j++){
			t[ j] = vectorize(xor5(devectorize(s[ j]),devectorize(s[j+5]),devectorize(s[j+10]),devectorize(s[j+15]),devectorize(s[j+20])));
		}
		/*theta*/
		#pragma unroll 5
		for(int j=0;j<5;j++){
			u[ j] = ROL2(t[ j], 1);
		}
		s[ 4] = xor3x(s[ 4], t[3], u[ 0]);s[ 9] = xor3x(s[ 9], t[3], u[ 0]);s[14] = xor3x(s[14], t[3], u[ 0]);s[19] = xor3x(s[19], t[3], u[ 0]);s[24] = xor3x(s[24], t[3], u[ 0]);
		s[ 0] = xor3x(s[ 0], t[4], u[ 1]);s[ 5] = xor3x(s[ 5], t[4], u[ 1]);s[10] = xor3x(s[10], t[4], u[ 1]);s[15] = xor3x(s[15], t[4], u[ 1]);s[20] = xor3x(s[20], t[4], u[ 1]);
		s[ 1] = xor3x(s[ 1], t[0], u[ 2]);s[ 6] = xor3x(s[ 6], t[0], u[ 2]);s[11] = xor3x(s[11], t[0], u[ 2]);s[16] = xor3x(s[16], t[0], u[ 2]);s[21] = xor3x(s[21], t[0], u[ 2]);
		s[ 2] = xor3x(s[ 2], t[1], u[ 3]);s[ 7] = xor3x(s[ 7], t[1], u[ 3]);s[12] = xor3x(s[12], t[1], u[ 3]);s[17] = xor3x(s[17], t[1], u[ 3]);s[22] = xor3x(s[22], t[1], u[ 3]);
		s[ 3] = xor3x(s[ 3], t[2], u[ 4]);s[ 8] = xor3x(s[ 8], t[2], u[ 4]);s[13] = xor3x(s[13], t[2], u[ 4]);s[18] = xor3x(s[18], t[2], u[ 4]);s[23] = xor3x(s[23], t[2], u[ 4]);

		/* rho pi: b[..] = rotl(a[..], ..) */
		v = s[1];
		s[1]  = ROL2(s[6], 44);
		s[6]  = ROL2(s[9], 20);
		s[9]  = ROL2(s[22], 61);
		s[22] = ROL2(s[14], 39);
		s[14] = ROL2(s[20], 18);
		s[20] = ROL2(s[2], 62);
		s[2]  = ROL2(s[12], 43);
		s[12] = ROL2(s[13], 25);
		s[13] = ROL8(s[19]);
		s[19] = ROR8(s[23]);
		s[23] = ROL2(s[15], 41);
		s[15] = ROL2(s[4], 27);
		s[4]  = ROL2(s[24], 14);
		s[24] = ROL2(s[21], 2);
		s[21] = ROL2(s[8], 55);
		s[8]  = ROL2(s[16], 45);
		s[16] = ROL2(s[5], 36);
		s[5]  = ROL2(s[3], 28);
		s[3]  = ROL2(s[18], 21);
		s[18] = ROL2(s[17], 15);
		s[17] = ROL2(s[11], 10);
		s[11] = ROL2(s[7], 6);
		s[7]  = ROL2(s[10], 3);
		s[10] = ROL2(v, 1);

		/* chi: a[i,j] ^= ~b[i,j+1] & b[i,j+2] */
		#pragma unroll 5
		for(int j=0;j<25;j+=5){
			v=s[j];w=s[j + 1];s[j] = chi(v,w,s[j+2]);s[j+1] = chi(w,s[j+2],s[j+3]);s[j+2]=chi(s[j+2],s[j+3],s[j+4]);s[j+3]=chi(s[j+3],s[j+4],v);s[j+4]=chi(s[j+4],v,w);
		}
		/* iota: a[0,0] ^= round constant */
		s[0] ^= keccak_round_constants[i];
	}
/*
	//theta
	#pragma unroll 5
	for(int j=0;j<5;j++){
		t[ j] = xor3x(xor3x(s[j+0],s[j+5],s[j+10]),s[j+15],s[j+20]);
	}
	//theta
	#pragma unroll 5
	for(int j=0;j<5;j++){
		u[ j] = ROL2(t[ j], 1);
	}
	s[ 9] = xor3x(s[ 9], t[3], u[ 0]);
	s[24] = xor3x(s[24], t[3], u[ 0]);
	s[ 0] = xor3x(s[ 0], t[4], u[ 1]);
	s[10] = xor3x(s[10], t[4], u[ 1]);
	s[ 6] = xor3x(s[ 6], t[0], u[ 2]);
	s[16] = xor3x(s[16], t[0], u[ 2]);
	s[12] = xor3x(s[12], t[1], u[ 3]);
	s[22] = xor3x(s[22], t[1], u[ 3]);
	s[ 3] = xor3x(s[ 3], t[2], u[ 4]);
	s[18] = xor3x(s[18], t[2], u[ 4]);
	// rho pi: b[..] = rotl(a[..], ..)
	s[ 1]  = ROL2(s[ 6], 44);
	s[ 2]  = ROL2(s[12], 43);
	s[ 5]  = ROL2(s[ 3], 28);
	s[ 7]  = ROL2(s[10], 3);
	s[ 3]  = ROL2(s[18], 21);
	s[ 4]  = ROL2(s[24], 14);
	s[ 6]  = ROL2(s[ 9], 20);
	s[ 8]  = ROL2(s[16], 45);
	s[ 9]  = ROL2(s[22], 61);
	// chi: a[i,j] ^= ~b[i,j+1] & b[i,j+2]
	v=s[ 0];w=s[ 1];s[ 0] = chi(v,w,s[ 2]);s[ 1] = chi(w,s[ 2],s[ 3]);s[ 2]=chi(s[ 2],s[ 3],s[ 4]);s[ 3]=chi(s[ 3],s[ 4],v);s[ 4]=chi(s[ 4],v,w);		
	v=s[ 5];w=s[ 6];s[ 5] = chi(v,w,s[ 7]);s[ 6] = chi(w,s[ 7],s[ 8]);s[ 7]=chi(s[ 7],s[ 8],s[ 9]);
	// iota: a[0,0] ^= round constant
	s[0] ^= keccak_round_constants[23];
*/
}

__device__ __forceinline__
static void streebog_kernel(const uint2 shared[8][256],uint2* s){

	uint2 buf[8], t[8], temp[8],K0[8];

	K0[0] = vectorize(0x74a5d4ce2efc83b3);

	#pragma unroll 8
	for(int i=0;i<8;i++){
		buf[ i] = K0[ 0] ^ s[ i];
	}

//	#pragma unroll 11
	for(int i=0; i<12; i++){
		GOST_FS(shared,buf, temp);
		#pragma unroll 8
		for(int j=0;j<8;j++){
			buf[ j] = temp[ j] ^ *(uint2*)&precomputed_values[i][j];
		}
	}
	#pragma unroll 8
	for(int j=0;j<8;j++){
		buf[ j]^= s[ j];
	}
	#pragma unroll 8
	for(int j=0;j<8;j++){
		K0[ j] = buf[ j];
	}
	
	K0[7].y ^= 0x00020000;
	
	GOST_FS(shared,K0, t);

	#pragma unroll 8
	for(int i=0;i<8;i++)
		K0[ i] = t[ i];
		
	t[7].y ^= 0x01000000;

	GOST_E12(shared,K0, t);

	#pragma unroll 8
	for(int j=0;j<8;j++)
		buf[ j] ^= t[ j];

	buf[7].y ^= 0x01000000;

	GOST_FS(shared,buf,K0);

	buf[7].y ^= 0x00020000;

	#pragma unroll 8
	for(int j=0;j<8;j++)
		t[ j] = K0[ j];
		
	t[7].y ^= 0x00020000;

	GOST_E12(shared,K0, t);

	#pragma unroll 8
	for(int j=0;j<8;j++)
		buf[ j] ^= t[ j];
		
	GOST_FS(shared,buf,K0); // K = F(h)

	s[7]+= vectorize(0x0100000000000000);

	#pragma unroll 8
	for(int j=0;j<8;j++)
		t[ j] = K0[ j] ^ s[ j];

	GOST_E12(shared,K0, t);
	
	#pragma unroll 8
	for(int i=0;i<8;i++)
		s[i] = s[ i] ^ buf[ i] ^ t[ i];
}

#define TPB 256
__global__
__launch_bounds__(TPB,3)
void keccak_streebog_gpu_hash_64(const uint32_t threads,uint64_t *g_hash)
{
	const uint32_t thread = (blockDim.x * blockIdx.x + threadIdx.x);

	uint2 s[25];
	
	__shared__ uint2 shared[8][256];
	shared[0][threadIdx.x] = __ldg(&T02[threadIdx.x]);
	shared[1][threadIdx.x] = __ldg(&T12[threadIdx.x]);
	shared[2][threadIdx.x] = __ldg(&T22[threadIdx.x]);
	shared[3][threadIdx.x] = __ldg(&T32[threadIdx.x]);
	shared[4][threadIdx.x] = __ldg(&T42[threadIdx.x]);
	shared[5][threadIdx.x] = __ldg(&T52[threadIdx.x]);
	shared[6][threadIdx.x] = __ldg(&T62[threadIdx.x]);
	shared[7][threadIdx.x] = __ldg(&T72[threadIdx.x]);

//	shared[threadIdx.x] = __ldg(&T02[threadIdx.x]);
//	shared[256+threadIdx.x] = __ldg(&T12[threadIdx.x]);
//	shared[512+threadIdx.x] = __ldg(&T22[threadIdx.x]);
//	shared[768+threadIdx.x] = __ldg(&T32[threadIdx.x]);
//	shared[1024+threadIdx.x] = __ldg(&T42[threadIdx.x]);

//	shared[1280+threadIdx.x] = T52[threadIdx.x];
//	shared[1536+threadIdx.x] = T62[threadIdx.x];
//	shared[1792+threadIdx.x] = T72[threadIdx.x];

	uint64_t* inout = &g_hash[thread<<3];

	__threadfence_block();
	
	*(uint2x4*)&s[ 0] = __ldg4((uint2x4 *)&inout[ 0]);
	*(uint2x4*)&s[ 4] = __ldg4((uint2x4 *)&inout[ 4]);
	s[8] = make_uint2(1,0x80000000);

	keccak_kernel(s);
	streebog_kernel(shared,s);
	
	*(uint2x4*)&inout[ 0] = *(uint2x4*)&s[ 0];
	*(uint2x4*)&inout[ 4] = *(uint2x4*)&s[ 4];
}

__host__
void keccak_streebog_cpu_hash_64(int thr_id, uint32_t threads, uint32_t *d_hash)
{
	dim3 grid((threads + TPB-1) / TPB);
	dim3 block(TPB);
	
	keccak_streebog_gpu_hash_64<<<grid, block>>>(threads,(uint64_t*)d_hash);
}

#define sph_u64 uint64_t



__constant__ static const sph_u64 RC[] = {
  SPH_C64(0x0000000000000001), SPH_C64(0x0000000000008082),
  SPH_C64(0x800000000000808A), SPH_C64(0x8000000080008000),
  SPH_C64(0x000000000000808B), SPH_C64(0x0000000080000001),
  SPH_C64(0x8000000080008081), SPH_C64(0x8000000000008009),
  SPH_C64(0x000000000000008A), SPH_C64(0x0000000000000088),
  SPH_C64(0x0000000080008009), SPH_C64(0x000000008000000A),
  SPH_C64(0x000000008000808B), SPH_C64(0x800000000000008B),
  SPH_C64(0x8000000000008089), SPH_C64(0x8000000000008003),
  SPH_C64(0x8000000000008002), SPH_C64(0x8000000000000080),
  SPH_C64(0x000000000000800A), SPH_C64(0x800000008000000A),
  SPH_C64(0x8000000080008081), SPH_C64(0x8000000000008080),
  SPH_C64(0x0000000080000001), SPH_C64(0x8000000080008008)
};

//#define SPH_ROTL64(x, n)   SPH_T64(((x) << (n)) | ((x) >> (64 - (n))))

#define SPH_ROTL64 ROTL64
#define DECL64(x)        sph_u64 x
#define MOV64(d, s)      (d = s)
#define XOR64(d, a, b)   (d = a ^ b)
#define AND64(d, a, b)   (d = a & b)
#define OR64(d, a, b)    (d = a | b)
#define NOT64(d, s)      (d = SPH_T64(~s))
#define ROL64(d, v, n)   (d = SPH_ROTL64(v, n))
#define XOR64_IOTA       XOR64


#define TH_ELT(t, c0, c1, c2, c3, c4, d0, d1, d2, d3, d4)   do { \
    DECL64(tt0); \
    DECL64(tt1); \
    DECL64(tt2); \
    DECL64(tt3); \
    XOR64(tt0, d0, d1); \
    XOR64(tt1, d2, d3); \
    XOR64(tt0, tt0, d4); \
    XOR64(tt0, tt0, tt1); \
    ROL64(tt0, tt0, 1); \
    XOR64(tt2, c0, c1); \
    XOR64(tt3, c2, c3); \
    XOR64(tt0, tt0, c4); \
    XOR64(tt2, tt2, tt3); \
    XOR64(t, tt0, tt2); \
  } while (0)

#define THETA(b00, b01, b02, b03, b04, b10, b11, b12, b13, b14, \
  b20, b21, b22, b23, b24, b30, b31, b32, b33, b34, \
  b40, b41, b42, b43, b44) \
  do { \
    DECL64(t0); \
    DECL64(t1); \
    DECL64(t2); \
    DECL64(t3); \
    DECL64(t4); \
    TH_ELT(t0, b40, b41, b42, b43, b44, b10, b11, b12, b13, b14); \
    TH_ELT(t1, b00, b01, b02, b03, b04, b20, b21, b22, b23, b24); \
    TH_ELT(t2, b10, b11, b12, b13, b14, b30, b31, b32, b33, b34); \
    TH_ELT(t3, b20, b21, b22, b23, b24, b40, b41, b42, b43, b44); \
    TH_ELT(t4, b30, b31, b32, b33, b34, b00, b01, b02, b03, b04); \
    XOR64(b00, b00, t0); \
    XOR64(b01, b01, t0); \
    XOR64(b02, b02, t0); \
    XOR64(b03, b03, t0); \
    XOR64(b04, b04, t0); \
    XOR64(b10, b10, t1); \
    XOR64(b11, b11, t1); \
    XOR64(b12, b12, t1); \
    XOR64(b13, b13, t1); \
    XOR64(b14, b14, t1); \
    XOR64(b20, b20, t2); \
    XOR64(b21, b21, t2); \
    XOR64(b22, b22, t2); \
    XOR64(b23, b23, t2); \
    XOR64(b24, b24, t2); \
    XOR64(b30, b30, t3); \
    XOR64(b31, b31, t3); \
    XOR64(b32, b32, t3); \
    XOR64(b33, b33, t3); \
    XOR64(b34, b34, t3); \
    XOR64(b40, b40, t4); \
    XOR64(b41, b41, t4); \
    XOR64(b42, b42, t4); \
    XOR64(b43, b43, t4); \
    XOR64(b44, b44, t4); \
  } while (0)

#define RHO(b00, b01, b02, b03, b04, b10, b11, b12, b13, b14, \
  b20, b21, b22, b23, b24, b30, b31, b32, b33, b34, \
  b40, b41, b42, b43, b44) \
  do { \
    /* ROL64(b00, b00,  0); */ \
    ROL64(b01, b01, 36); \
    ROL64(b02, b02,  3); \
    ROL64(b03, b03, 41); \
    ROL64(b04, b04, 18); \
    ROL64(b10, b10,  1); \
    ROL64(b11, b11, 44); \
    ROL64(b12, b12, 10); \
    ROL64(b13, b13, 45); \
    ROL64(b14, b14,  2); \
    ROL64(b20, b20, 62); \
    ROL64(b21, b21,  6); \
    ROL64(b22, b22, 43); \
    ROL64(b23, b23, 15); \
    ROL64(b24, b24, 61); \
    ROL64(b30, b30, 28); \
    ROL64(b31, b31, 55); \
    ROL64(b32, b32, 25); \
    ROL64(b33, b33, 21); \
    ROL64(b34, b34, 56); \
    ROL64(b40, b40, 27); \
    ROL64(b41, b41, 20); \
    ROL64(b42, b42, 39); \
    ROL64(b43, b43,  8); \
    ROL64(b44, b44, 14); \
  } while (0)

/*
 * The KHI macro integrates the "lane complement" optimization. On input,
 * some words are complemented:
 *    a00 a01 a02 a04 a13 a20 a21 a22 a30 a33 a34 a43
 * On output, the following words are complemented:
 *    a04 a10 a20 a22 a23 a31
 *
 * The (implicit) permutation and the theta expansion will bring back
 * the input mask for the next round.
 */

#define KHI_XO(d, a, b, c)   do { \
    DECL64(kt); \
    OR64(kt, b, c); \
    XOR64(d, a, kt); \
  } while (0)

#define KHI_XA(d, a, b, c)   do { \
    DECL64(kt); \
    AND64(kt, b, c); \
    XOR64(d, a, kt); \
  } while (0)

#define KHI(b00, b01, b02, b03, b04, b10, b11, b12, b13, b14, \
  b20, b21, b22, b23, b24, b30, b31, b32, b33, b34, \
  b40, b41, b42, b43, b44) \
  do { \
    DECL64(c0); \
    DECL64(c1); \
    DECL64(c2); \
    DECL64(c3); \
    DECL64(c4); \
    DECL64(bnn); \
    NOT64(bnn, b20); \
    KHI_XO(c0, b00, b10, b20); \
    KHI_XO(c1, b10, bnn, b30); \
    KHI_XA(c2, b20, b30, b40); \
    KHI_XO(c3, b30, b40, b00); \
    KHI_XA(c4, b40, b00, b10); \
    MOV64(b00, c0); \
    MOV64(b10, c1); \
    MOV64(b20, c2); \
    MOV64(b30, c3); \
    MOV64(b40, c4); \
    NOT64(bnn, b41); \
    KHI_XO(c0, b01, b11, b21); \
    KHI_XA(c1, b11, b21, b31); \
    KHI_XO(c2, b21, b31, bnn); \
    KHI_XO(c3, b31, b41, b01); \
    KHI_XA(c4, b41, b01, b11); \
    MOV64(b01, c0); \
    MOV64(b11, c1); \
    MOV64(b21, c2); \
    MOV64(b31, c3); \
    MOV64(b41, c4); \
    NOT64(bnn, b32); \
    KHI_XO(c0, b02, b12, b22); \
    KHI_XA(c1, b12, b22, b32); \
    KHI_XA(c2, b22, bnn, b42); \
    KHI_XO(c3, bnn, b42, b02); \
    KHI_XA(c4, b42, b02, b12); \
    MOV64(b02, c0); \
    MOV64(b12, c1); \
    MOV64(b22, c2); \
    MOV64(b32, c3); \
    MOV64(b42, c4); \
    NOT64(bnn, b33); \
    KHI_XA(c0, b03, b13, b23); \
    KHI_XO(c1, b13, b23, b33); \
    KHI_XO(c2, b23, bnn, b43); \
    KHI_XA(c3, bnn, b43, b03); \
    KHI_XO(c4, b43, b03, b13); \
    MOV64(b03, c0); \
    MOV64(b13, c1); \
    MOV64(b23, c2); \
    MOV64(b33, c3); \
    MOV64(b43, c4); \
    NOT64(bnn, b14); \
    KHI_XA(c0, b04, bnn, b24); \
    KHI_XO(c1, bnn, b24, b34); \
    KHI_XA(c2, b24, b34, b44); \
    KHI_XO(c3, b34, b44, b04); \
    KHI_XA(c4, b44, b04, b14); \
    MOV64(b04, c0); \
    MOV64(b14, c1); \
    MOV64(b24, c2); \
    MOV64(b34, c3); \
    MOV64(b44, c4); \
  } while (0)

#define IOTA(r)   XOR64_IOTA(a00, a00, r)

#define P0    a00, a01, a02, a03, a04, a10, a11, a12, a13, a14, a20, a21, \
        a22, a23, a24, a30, a31, a32, a33, a34, a40, a41, a42, a43, a44
#define P1    a00, a30, a10, a40, a20, a11, a41, a21, a01, a31, a22, a02, \
        a32, a12, a42, a33, a13, a43, a23, a03, a44, a24, a04, a34, a14
#define P2    a00, a33, a11, a44, a22, a41, a24, a02, a30, a13, a32, a10, \
        a43, a21, a04, a23, a01, a34, a12, a40, a14, a42, a20, a03, a31
#define P3    a00, a23, a41, a14, a32, a24, a42, a10, a33, a01, a43, a11, \
        a34, a02, a20, a12, a30, a03, a21, a44, a31, a04, a22, a40, a13
#define P4    a00, a12, a24, a31, a43, a42, a04, a11, a23, a30, a34, a41, \
        a03, a10, a22, a21, a33, a40, a02, a14, a13, a20, a32, a44, a01
#define P5    a00, a21, a42, a13, a34, a04, a20, a41, a12, a33, a03, a24, \
        a40, a11, a32, a02, a23, a44, a10, a31, a01, a22, a43, a14, a30
#define P6    a00, a02, a04, a01, a03, a20, a22, a24, a21, a23, a40, a42, \
        a44, a41, a43, a10, a12, a14, a11, a13, a30, a32, a34, a31, a33
#define P7    a00, a10, a20, a30, a40, a22, a32, a42, a02, a12, a44, a04, \
        a14, a24, a34, a11, a21, a31, a41, a01, a33, a43, a03, a13, a23
#define P8    a00, a11, a22, a33, a44, a32, a43, a04, a10, a21, a14, a20, \
        a31, a42, a03, a41, a02, a13, a24, a30, a23, a34, a40, a01, a12
#define P9    a00, a41, a32, a23, a14, a43, a34, a20, a11, a02, a31, a22, \
        a13, a04, a40, a24, a10, a01, a42, a33, a12, a03, a44, a30, a21
#define P10   a00, a24, a43, a12, a31, a34, a03, a22, a41, a10, a13, a32, \
        a01, a20, a44, a42, a11, a30, a04, a23, a21, a40, a14, a33, a02
#define P11   a00, a42, a34, a21, a13, a03, a40, a32, a24, a11, a01, a43, \
        a30, a22, a14, a04, a41, a33, a20, a12, a02, a44, a31, a23, a10
#define P12   a00, a04, a03, a02, a01, a40, a44, a43, a42, a41, a30, a34, \
        a33, a32, a31, a20, a24, a23, a22, a21, a10, a14, a13, a12, a11
#define P13   a00, a20, a40, a10, a30, a44, a14, a34, a04, a24, a33, a03, \
        a23, a43, a13, a22, a42, a12, a32, a02, a11, a31, a01, a21, a41
#define P14   a00, a22, a44, a11, a33, a14, a31, a03, a20, a42, a23, a40, \
        a12, a34, a01, a32, a04, a21, a43, a10, a41, a13, a30, a02, a24
#define P15   a00, a32, a14, a41, a23, a31, a13, a40, a22, a04, a12, a44, \
        a21, a03, a30, a43, a20, a02, a34, a11, a24, a01, a33, a10, a42
#define P16   a00, a43, a31, a24, a12, a13, a01, a44, a32, a20, a21, a14, \
        a02, a40, a33, a34, a22, a10, a03, a41, a42, a30, a23, a11, a04
#define P17   a00, a34, a13, a42, a21, a01, a30, a14, a43, a22, a02, a31, \
        a10, a44, a23, a03, a32, a11, a40, a24, a04, a33, a12, a41, a20
#define P18   a00, a03, a01, a04, a02, a30, a33, a31, a34, a32, a10, a13, \
        a11, a14, a12, a40, a43, a41, a44, a42, a20, a23, a21, a24, a22
#define P19   a00, a40, a30, a20, a10, a33, a23, a13, a03, a43, a11, a01, \
        a41, a31, a21, a44, a34, a24, a14, a04, a22, a12, a02, a42, a32
#define P20   a00, a44, a33, a22, a11, a23, a12, a01, a40, a34, a41, a30, \
        a24, a13, a02, a14, a03, a42, a31, a20, a32, a21, a10, a04, a43
#define P21   a00, a14, a23, a32, a41, a12, a21, a30, a44, a03, a24, a33, \
        a42, a01, a10, a31, a40, a04, a13, a22, a43, a02, a11, a20, a34
#define P22   a00, a31, a12, a43, a24, a21, a02, a33, a14, a40, a42, a23, \
        a04, a30, a11, a13, a44, a20, a01, a32, a34, a10, a41, a22, a03
#define P23   a00, a13, a21, a34, a42, a02, a10, a23, a31, a44, a04, a12, \
        a20, a33, a41, a01, a14, a22, a30, a43, a03, a11, a24, a32, a40

#define P1_TO_P0   do { \
    DECL64(t); \
    MOV64(t, a01); \
    MOV64(a01, a30); \
    MOV64(a30, a33); \
    MOV64(a33, a23); \
    MOV64(a23, a12); \
    MOV64(a12, a21); \
    MOV64(a21, a02); \
    MOV64(a02, a10); \
    MOV64(a10, a11); \
    MOV64(a11, a41); \
    MOV64(a41, a24); \
    MOV64(a24, a42); \
    MOV64(a42, a04); \
    MOV64(a04, a20); \
    MOV64(a20, a22); \
    MOV64(a22, a32); \
    MOV64(a32, a43); \
    MOV64(a43, a34); \
    MOV64(a34, a03); \
    MOV64(a03, a40); \
    MOV64(a40, a44); \
    MOV64(a44, a14); \
    MOV64(a14, a31); \
    MOV64(a31, a13); \
    MOV64(a13, t); \
  } while (0)

#define P2_TO_P0   do { \
    DECL64(t); \
    MOV64(t, a01); \
    MOV64(a01, a33); \
    MOV64(a33, a12); \
    MOV64(a12, a02); \
    MOV64(a02, a11); \
    MOV64(a11, a24); \
    MOV64(a24, a04); \
    MOV64(a04, a22); \
    MOV64(a22, a43); \
    MOV64(a43, a03); \
    MOV64(a03, a44); \
    MOV64(a44, a31); \
    MOV64(a31, t); \
    MOV64(t, a10); \
    MOV64(a10, a41); \
    MOV64(a41, a42); \
    MOV64(a42, a20); \
    MOV64(a20, a32); \
    MOV64(a32, a34); \
    MOV64(a34, a40); \
    MOV64(a40, a14); \
    MOV64(a14, a13); \
    MOV64(a13, a30); \
    MOV64(a30, a23); \
    MOV64(a23, a21); \
    MOV64(a21, t); \
  } while (0)

#define P4_TO_P0   do { \
    DECL64(t); \
    MOV64(t, a01); \
    MOV64(a01, a12); \
    MOV64(a12, a11); \
    MOV64(a11, a04); \
    MOV64(a04, a43); \
    MOV64(a43, a44); \
    MOV64(a44, t); \
    MOV64(t, a02); \
    MOV64(a02, a24); \
    MOV64(a24, a22); \
    MOV64(a22, a03); \
    MOV64(a03, a31); \
    MOV64(a31, a33); \
    MOV64(a33, t); \
    MOV64(t, a10); \
    MOV64(a10, a42); \
    MOV64(a42, a32); \
    MOV64(a32, a40); \
    MOV64(a40, a13); \
    MOV64(a13, a23); \
    MOV64(a23, t); \
    MOV64(t, a14); \
    MOV64(a14, a30); \
    MOV64(a30, a21); \
    MOV64(a21, a41); \
    MOV64(a41, a20); \
    MOV64(a20, a34); \
    MOV64(a34, t); \
  } while (0)

#define P6_TO_P0   do { \
    DECL64(t); \
    MOV64(t, a01); \
    MOV64(a01, a02); \
    MOV64(a02, a04); \
    MOV64(a04, a03); \
    MOV64(a03, t); \
    MOV64(t, a10); \
    MOV64(a10, a20); \
    MOV64(a20, a40); \
    MOV64(a40, a30); \
    MOV64(a30, t); \
    MOV64(t, a11); \
    MOV64(a11, a22); \
    MOV64(a22, a44); \
    MOV64(a44, a33); \
    MOV64(a33, t); \
    MOV64(t, a12); \
    MOV64(a12, a24); \
    MOV64(a24, a43); \
    MOV64(a43, a31); \
    MOV64(a31, t); \
    MOV64(t, a13); \
    MOV64(a13, a21); \
    MOV64(a21, a42); \
    MOV64(a42, a34); \
    MOV64(a34, t); \
    MOV64(t, a14); \
    MOV64(a14, a23); \
    MOV64(a23, a41); \
    MOV64(a41, a32); \
    MOV64(a32, t); \
  } while (0)

#define P8_TO_P0   do { \
    DECL64(t); \
    MOV64(t, a01); \
    MOV64(a01, a11); \
    MOV64(a11, a43); \
    MOV64(a43, t); \
    MOV64(t, a02); \
    MOV64(a02, a22); \
    MOV64(a22, a31); \
    MOV64(a31, t); \
    MOV64(t, a03); \
    MOV64(a03, a33); \
    MOV64(a33, a24); \
    MOV64(a24, t); \
    MOV64(t, a04); \
    MOV64(a04, a44); \
    MOV64(a44, a12); \
    MOV64(a12, t); \
    MOV64(t, a10); \
    MOV64(a10, a32); \
    MOV64(a32, a13); \
    MOV64(a13, t); \
    MOV64(t, a14); \
    MOV64(a14, a21); \
    MOV64(a21, a20); \
    MOV64(a20, t); \
    MOV64(t, a23); \
    MOV64(a23, a42); \
    MOV64(a42, a40); \
    MOV64(a40, t); \
    MOV64(t, a30); \
    MOV64(a30, a41); \
    MOV64(a41, a34); \
    MOV64(a34, t); \
  } while (0)

#define P12_TO_P0   do { \
    DECL64(t); \
    MOV64(t, a01); \
    MOV64(a01, a04); \
    MOV64(a04, t); \
    MOV64(t, a02); \
    MOV64(a02, a03); \
    MOV64(a03, t); \
    MOV64(t, a10); \
    MOV64(a10, a40); \
    MOV64(a40, t); \
    MOV64(t, a11); \
    MOV64(a11, a44); \
    MOV64(a44, t); \
    MOV64(t, a12); \
    MOV64(a12, a43); \
    MOV64(a43, t); \
    MOV64(t, a13); \
    MOV64(a13, a42); \
    MOV64(a42, t); \
    MOV64(t, a14); \
    MOV64(a14, a41); \
    MOV64(a41, t); \
    MOV64(t, a20); \
    MOV64(a20, a30); \
    MOV64(a30, t); \
    MOV64(t, a21); \
    MOV64(a21, a34); \
    MOV64(a34, t); \
    MOV64(t, a22); \
    MOV64(a22, a33); \
    MOV64(a33, t); \
    MOV64(t, a23); \
    MOV64(a23, a32); \
    MOV64(a32, t); \
    MOV64(t, a24); \
    MOV64(a24, a31); \
    MOV64(a31, t); \
  } while (0)

#define LPAR   (
#define RPAR   )

#define KF_ELT(r, s, k)   do { \
    THETA LPAR P ## r RPAR; \
    RHO LPAR P ## r RPAR; \
    KHI LPAR P ## s RPAR; \
IOTA(k); \
  } while (0)


//    IOTA(k); \

#define DO(x)   x
#define KECCAK_F_1600   DO(KECCAK_F_1600_)
#define KECCAK_F_1600_   do { \
    int j; \
    for (j = 0; j < 24; j ++) { \
      KF_ELT( 0,  1, RC[j + 0]); \
      P1_TO_P0; \
    } \
  } while (0)

//      P1_TO_P0; \

#define U32TO64_LE(p) \
	(((uint64_t)(*p)) | (((uint64_t)(*(p + 1))) << 32))

#define U64TO32_LE(p, v) \
	*p = (uint32_t)((v)); *(p+1) = (uint32_t)((v) >> 32);

static const uint64_t host_keccak_round_constants[24] = {
	0x0000000000000001ull, 0x0000000000008082ull,
	0x800000000000808aull, 0x8000000080008000ull,
	0x000000000000808bull, 0x0000000080000001ull,
	0x8000000080008081ull, 0x8000000000008009ull,
	0x000000000000008aull, 0x0000000000000088ull,
	0x0000000080008009ull, 0x000000008000000aull,
	0x000000008000808bull, 0x800000000000008bull,
	0x8000000000008089ull, 0x8000000000008003ull,
	0x8000000000008002ull, 0x8000000000000080ull,
	0x000000000000800aull, 0x800000008000000aull,
	0x8000000080008081ull, 0x8000000000008080ull,
	0x0000000080000001ull, 0x8000000080008008ull
};

__constant__ uint64_t d_keccak_round_constants[24];

__device__ __forceinline__
static void keccak_block(uint2 *s)
{
	size_t i;
	uint2 t[5], u[5], v, w;

	for (i = 0; i < 24; i++) {
		/* theta: c = a[0,i] ^ a[1,i] ^ .. a[4,i] */
		t[0] = s[0] ^ s[5] ^ s[10] ^ s[15] ^ s[20];
		t[1] = s[1] ^ s[6] ^ s[11] ^ s[16] ^ s[21];
		t[2] = s[2] ^ s[7] ^ s[12] ^ s[17] ^ s[22];
		t[3] = s[3] ^ s[8] ^ s[13] ^ s[18] ^ s[23];
		t[4] = s[4] ^ s[9] ^ s[14] ^ s[19] ^ s[24];

		/* theta: d[i] = c[i+4] ^ rotl(c[i+1],1) */
		u[0] = t[4] ^ ROL2(t[1], 1);
		u[1] = t[0] ^ ROL2(t[2], 1);
		u[2] = t[1] ^ ROL2(t[3], 1);
		u[3] = t[2] ^ ROL2(t[4], 1);
		u[4] = t[3] ^ ROL2(t[0], 1);

		/* theta: a[0,i], a[1,i], .. a[4,i] ^= d[i] */
		s[0] ^= u[0]; s[5] ^= u[0]; s[10] ^= u[0]; s[15] ^= u[0]; s[20] ^= u[0];
		s[1] ^= u[1]; s[6] ^= u[1]; s[11] ^= u[1]; s[16] ^= u[1]; s[21] ^= u[1];
		s[2] ^= u[2]; s[7] ^= u[2]; s[12] ^= u[2]; s[17] ^= u[2]; s[22] ^= u[2];
		s[3] ^= u[3]; s[8] ^= u[3]; s[13] ^= u[3]; s[18] ^= u[3]; s[23] ^= u[3];
		s[4] ^= u[4]; s[9] ^= u[4]; s[14] ^= u[4]; s[19] ^= u[4]; s[24] ^= u[4];

		/* rho pi: b[..] = rotl(a[..], ..) */
		v = s[1];
		s[1]  = ROL2(s[6], 44);
		s[6]  = ROL2(s[9], 20);
		s[9]  = ROL2(s[22], 61);
		s[22] = ROL2(s[14], 39);
		s[14] = ROL2(s[20], 18);
		s[20] = ROL2(s[2], 62);
		s[2]  = ROL2(s[12], 43);
		s[12] = ROL2(s[13], 25);
		s[13] = ROL2(s[19], 8);
		s[19] = ROL2(s[23], 56);
		s[23] = ROL2(s[15], 41);
		s[15] = ROL2(s[4], 27);
		s[4]  = ROL2(s[24], 14);
		s[24] = ROL2(s[21], 2);
		s[21] = ROL2(s[8], 55);
		s[8]  = ROL2(s[16], 45);
		s[16] = ROL2(s[5], 36);
		s[5]  = ROL2(s[3], 28);
		s[3]  = ROL2(s[18], 21);
		s[18] = ROL2(s[17], 15);
		s[17] = ROL2(s[11], 10);
		s[11] = ROL2(s[7], 6);
		s[7]  = ROL2(s[10], 3);
		s[10] = ROL2(v, 1);

		/* chi: a[i,j] ^= ~b[i,j+1] & b[i,j+2] */
		v = s[0]; w = s[1]; s[0] ^= (~w) & s[2]; s[1] ^= (~s[2]) & s[3]; s[2] ^= (~s[3]) & s[4]; s[3] ^= (~s[4]) & v; s[4] ^= (~v) & w;
		v = s[5]; w = s[6]; s[5] ^= (~w) & s[7]; s[6] ^= (~s[7]) & s[8]; s[7] ^= (~s[8]) & s[9]; s[8] ^= (~s[9]) & v; s[9] ^= (~v) & w;
		v = s[10]; w = s[11]; s[10] ^= (~w) & s[12]; s[11] ^= (~s[12]) & s[13]; s[12] ^= (~s[13]) & s[14]; s[13] ^= (~s[14]) & v; s[14] ^= (~v) & w;
		v = s[15]; w = s[16]; s[15] ^= (~w) & s[17]; s[16] ^= (~s[17]) & s[18]; s[17] ^= (~s[18]) & s[19]; s[18] ^= (~s[19]) & v; s[19] ^= (~v) & w;
		v = s[20]; w = s[21]; s[20] ^= (~w) & s[22]; s[21] ^= (~s[22]) & s[23]; s[22] ^= (~s[23]) & s[24]; s[23] ^= (~s[24]) & v; s[24] ^= (~v) & w;

		/* iota: a[0,0] ^= round constant */
//		s[0] ^= vectorize(d_keccak_round_constants[i]);
	}
}


#define TPB 256
__global__
__launch_bounds__(TPB,3)
void keccak_xevan_gpu_hash_64(const uint32_t threads,uint64_t *g_hash)
{
        const uint32_t thread = (blockDim.x * blockIdx.x + threadIdx.x);

        uint64_t s[8];
        

        uint64_t* inout = &g_hash[thread<<3];

        *(uint2x4*)&s[ 0] = __ldg4((uint2x4 *)&inout[ 0]);
        *(uint2x4*)&s[ 4] = __ldg4((uint2x4 *)&inout[ 4]);


sph_u64 a00 = 0, a01 = 0, a02 = 0, a03 = 0, a04 = 0;
  sph_u64 a10 = 0, a11 = 0, a12 = 0, a13 = 0, a14 = 0;
  sph_u64 a20 = 0, a21 = 0, a22 = 0, a23 = 0, a24 = 0;
  sph_u64 a30 = 0, a31 = 0, a32 = 0, a33 = 0, a34 = 0;
  sph_u64 a40 = 0, a41 = 0, a42 = 0, a43 = 0, a44 = 0;

  a10 = SPH_C64(0xFFFFFFFFFFFFFFFF);
  a20 = SPH_C64(0xFFFFFFFFFFFFFFFF);
  a31 = SPH_C64(0xFFFFFFFFFFFFFFFF);
  a22 = SPH_C64(0xFFFFFFFFFFFFFFFF);
  a23 = SPH_C64(0xFFFFFFFFFFFFFFFF);
  a04 = SPH_C64(0xFFFFFFFFFFFFFFFF);

  a00 ^= s[0];
  a10 ^= s[1];
  a20 ^= s[2];
  a30 ^= s[3];
  a40 ^= s[4];
  a01 ^= s[5];
  a11 ^= s[6];
  a21 ^= s[7];
//#pragma unroll 
  KECCAK_F_1600;

/*
  s[0] = (a00);
  s[1] = (a10);
  s[2] = (a20);
  s[3] = (a30);
  s[4] = (a40);
  s[5] = (a01);
  s[6] = (a11);
  s[7] = (a21);

*/


  a21 ^= 0x1;
  a31 ^= 0x8000000000000000;
//  KECCAK_F_1600;

  // Finalize the "lane complement"
  //a10 = ~a10;
  //a20 = ~a20;

  s[0] = (a00);
  s[1] = (a10);
  s[2] = (a20);
  s[3] = (a30);
  s[4] = (a40);
  s[5] = (a01);
  s[6] = (a11);
  s[7] = (a21);
//  s[0] = (a31);
        

        *(uint2x4*)&inout[ 0] = *(uint2x4*)&s[ 0];
        *(uint2x4*)&inout[ 4] = *(uint2x4*)&s[ 4];
}

__host__
void keccak_xevan_cpu_hash_64(int thr_id, uint32_t threads, uint32_t *d_hash)
{
        dim3 grid((threads + TPB-1) / TPB);
        dim3 block(TPB);
        
        keccak_xevan_gpu_hash_64<<<grid, block>>>(threads,(uint64_t*)d_hash);
}



__global__
__launch_bounds__(TPB,3)
void keccak_xevan_gpu_hash_64_A(const uint32_t threads,uint64_t *g_hash)
{
        const uint32_t thread = (blockDim.x * blockIdx.x + threadIdx.x);

        uint64_t s[8];
        

        uint64_t* inout = &g_hash[thread<<3];

        *(uint2x4*)&s[ 0] = __ldg4((uint2x4 *)&inout[ 0]);
        *(uint2x4*)&s[ 4] = __ldg4((uint2x4 *)&inout[ 4]);
	uint2 keccak_gpu_state[25];


	for (int i = 0; i<8; i++) {
			keccak_gpu_state[i] = vectorize(s[i]);
		}

                for (int i=8; i<25; i++) {
                        keccak_gpu_state[i] = make_uint2(0, 0);
                }

	keccak_kernel(keccak_gpu_state);
keccak_gpu_state[1] = ~keccak_gpu_state[1];
keccak_gpu_state[2] = ~keccak_gpu_state[2];
keccak_gpu_state[8] = ~keccak_gpu_state[8];

keccak_gpu_state[7] ^= vectorize(0x1UL);
keccak_gpu_state[8] ^= vectorize(0x8000000000000000UL);
//keccak_gpu_state[8] = make_uint2(1,0x80000000);

    //    keccak_kernel(keccak_gpu_state);

		for(int i=0; i<8; i++) {
			s[i] = devectorize(keccak_gpu_state[i]);
		}


//s[0] = devectorize(~keccak_gpu_state[8]);

        *(uint2x4*)&inout[ 0] = *(uint2x4*)&s[ 0];
        *(uint2x4*)&inout[ 4] = *(uint2x4*)&s[ 4];
}


__host__
void keccak_xevan_cpu_hash_64_A(int thr_id, uint32_t threads, uint32_t *d_hash)
{
        dim3 grid((threads + TPB-1) / TPB);
        dim3 block(TPB);
        
        keccak_xevan_gpu_hash_64_A<<<grid, block>>>(threads,(uint64_t*)d_hash);
}
